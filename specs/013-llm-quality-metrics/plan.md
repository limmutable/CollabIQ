# Implementation Plan: LLM Quality Metrics & Tracking

**Branch**: `013-llm-quality-metrics` | **Date**: 2025-11-09 | **Spec**: [spec.md](spec.md)
**Input**: Feature specification from `/specs/013-llm-quality-metrics/spec.md`

**Note**: This template is filled in by the `/speckit.plan` command. See `.specify/templates/commands/plan.md` for the execution workflow.

## Summary

This feature adds comprehensive quality metrics tracking for LLM providers, enabling administrators to monitor how accurately each provider extracts the 5 key entities (person_in_charge, startup_name, partner_org, details, date) from emails and matches them to Notion database fields. The system tracks per-field confidence scores, overall extraction confidence, field completeness (% of extractions with all 5 fields populated), Notion matching accuracy, and validation failures. Administrators can compare provider performance, calculate quality-to-cost ratios, and implement quality-based routing decisions. This builds upon the existing health tracking and cost tracking infrastructure to provide a complete observability layer for LLM operations, ensuring high-quality data is written to Notion.

## Technical Context

**Language/Version**: Python 3.12 (established in project)
**Primary Dependencies**: pydantic (data validation), existing LLM adapters (gemini_adapter, claude_adapter, openai_adapter), existing health_tracker and cost_tracker modules
**Storage**: File-based JSON persistence in `data/llm_health/` directory (consistent with existing health_metrics.json and cost_metrics.json patterns)
**Testing**: pytest (established in project) with contract tests for quality tracker API, integration tests for end-to-end quality tracking workflow
**Target Platform**: Linux/macOS server environment (Python runtime)
**Project Type**: Single project (backend service)
**Performance Goals**: Query quality metrics within 2 seconds for 10,000+ extraction records, atomic file writes for concurrent safety
**Constraints**: Must integrate with existing LLM orchestrator failover logic, maintain consistency with health_tracker and cost_tracker patterns, support CLI access via existing admin CLI
**Scale/Scope**: Track metrics for 3 LLM providers (Gemini, Claude, OpenAI), support 10,000+ extractions per provider, calculate statistical trends and comparisons

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

### I. Specification-First Development ✅ PASS

- ✅ Feature specification ([spec.md](spec.md)) exists and is complete
- ✅ User scenarios with acceptance criteria defined (3 user stories with 9 acceptance scenarios)
- ✅ Functional requirements documented (FR-001 through FR-010)
- ✅ Success criteria defined (SC-001 through SC-006)
- ✅ All requirements are technology-agnostic in the specification

**Status**: PASS - Specification is complete before implementation begins.

### II. Incremental Delivery via Independent User Stories ✅ PASS

- ✅ User stories are prioritized (P1, P2, P3)
- ✅ Each user story is independently testable
  - **P1**: Track quality metrics - can be tested by recording and retrieving metrics
  - **P2**: Compare providers - can be tested by comparing metrics across providers
  - **P3**: Quality-based routing - can be tested by configuring thresholds and verifying routing
- ✅ P1 constitutes viable MVP (basic quality tracking delivers immediate value)
- ✅ Implementation will proceed in priority order (P1 → P2 → P3)
- ✅ Each story completion results in deployable increment

**Status**: PASS - User stories are properly prioritized and independently deliverable.

### III. Test-Driven Development (TDD) ⚠️ CONDITIONAL

- ⚠️ Tests are not explicitly required in the feature specification
- ✅ If tests are written, TDD will be followed (test-first discipline)
- ✅ Contract tests identified for quality tracker API
- ✅ Integration tests identified for end-to-end quality tracking

**Status**: CONDITIONAL PASS - Tests are optional per constitution ("Tests are OPTIONAL unless explicitly requested"). If tests are written during implementation, strict TDD will be enforced.

### IV. Design Artifact Completeness ⏳ IN PROGRESS

Planning phase artifacts:
- ✅ `spec.md` - Feature specification exists
- ✅ `plan.md` - This file (in progress)
- ⏳ `research.md` - To be generated in Phase 0
- ⏳ `data-model.md` - To be generated in Phase 1
- ⏳ `quickstart.md` - To be generated in Phase 1
- ⏳ `contracts/` - To be generated in Phase 1
- ⏳ `tasks.md` - Will be generated by `/speckit.tasks` after planning complete

**Status**: IN PROGRESS - Planning artifacts will be completed before task generation.

### V. Simplicity & Justification ✅ PASS

- ✅ Solution uses existing patterns (file-based JSON persistence like health_tracker and cost_tracker)
- ✅ No new frameworks or abstractions introduced
- ✅ Complexity is justified: quality tracking requires statistical calculations (average, standard deviation, trends) which are domain requirements, not premature optimization
- ✅ YAGNI principle followed: building only what is specified (quality metrics, comparison, routing integration)

**Status**: PASS - Solution follows simplest approach consistent with existing codebase patterns.

### Overall Gate Status: ✅ PASS

**Initial Check**: All constitution principles satisfied. Planning proceeded to Phase 0 (Research).

**Post-Design Re-evaluation** (2025-11-09 after Phase 1):

#### IV. Design Artifact Completeness ✅ COMPLETE

Planning phase artifacts:
- ✅ `spec.md` - Feature specification complete (updated with 5-entity extraction clarity)
- ✅ `plan.md` - This file complete (Technical Context, Constitution Check, Project Structure)
- ✅ `research.md` - Phase 0 complete (6 research tasks resolved, all technical decisions documented)
- ✅ `data-model.md` - Phase 1 complete (4 core entities defined: QualityMetricsRecord, ProviderQualitySummary, QualityThresholdConfig, ProviderQualityComparison)
- ✅ `quickstart.md` - Phase 1 complete (7-step user guide with CLI examples)
- ✅ `contracts/` - Phase 1 complete (QualityTracker API contract with 6 public methods documented)
- ⏳ `tasks.md` - Will be generated by `/speckit.tasks` after planning approved

**Status**: All planning artifacts complete. Ready for task generation (`/speckit.tasks`).

#### V. Simplicity & Justification ✅ CONFIRMED POST-DESIGN

Design review confirms adherence to simplicity principle:
- ✅ No new frameworks introduced (uses existing pydantic, pytest)
- ✅ Follows existing patterns (QualityTracker mirrors HealthTracker and CostTracker structure)
- ✅ File-based persistence consistent with existing approach (quality_metrics.json alongside health_metrics.json and cost_metrics.json)
- ✅ Atomic write pattern reused from HealthTracker (tempfile + rename)
- ✅ Integration point is single method call in orchestrator (after cost tracking)
- ✅ Statistical calculations justified by requirements (trend detection, value score for comparison)
- ✅ No premature optimization (aggregates only, no individual record storage for MVP)

**Status**: Design maintains simplicity, all complexity justified by requirements.

### Final Gate Status: ✅ PASS - Ready for Task Generation

All constitution principles satisfied after Phase 0 (Research) and Phase 1 (Design) completion.

## Project Structure

### Documentation (this feature)

```text
specs/013-llm-quality-metrics/
├── spec.md              # Feature specification (completed)
├── plan.md              # This file (/speckit.plan command output)
├── research.md          # Phase 0 output (/speckit.plan command)
├── data-model.md        # Phase 1 output (/speckit.plan command)
├── quickstart.md        # Phase 1 output (/speckit.plan command)
├── contracts/           # Phase 1 output (/speckit.plan command)
├── checklists/          # Quality validation checklists
│   └── requirements.md  # Specification quality checklist (completed)
└── tasks.md             # Phase 2 output (/speckit.tasks command - NOT created by /speckit.plan)
```

### Source Code (repository root)

```text
src/
├── llm_orchestrator/
│   ├── quality_tracker.py          # NEW: Quality metrics tracking (mirroring health_tracker pattern)
│   ├── types.py                     # EXTEND: Add QualityMetricsRecord, ProviderQualitySummary
│   ├── cost_tracker.py              # EXISTING: Will integrate with quality-to-cost calculations
│   └── orchestrator.py              # EXTEND: Add quality-based routing logic
├── llm_adapters/
│   ├── base_adapter.py              # REVIEW: Ensure confidence scores are captured
│   ├── gemini_adapter.py            # EXTEND: Extract and return confidence scores
│   ├── claude_adapter.py            # EXTEND: Extract and return confidence scores
│   ├── openai_adapter.py            # EXTEND: Extract and return confidence scores
│   ├── health_tracker.py            # EXISTING: Reference pattern for quality_tracker
│   └── types.py                     # EXTEND: Add confidence score fields to response types
├── cli/
│   └── commands/
│       └── status.py                # EXTEND: Add quality metrics display to status command
└── content_normalizer/
    └── validator.py                 # REVIEW: Leverage for validation failure tracking

data/
└── llm_health/
    ├── health_metrics.json          # EXISTING: Health tracking data
    ├── cost_metrics.json            # EXISTING: Cost tracking data
    └── quality_metrics.json         # NEW: Quality tracking data

tests/
├── contract/
│   └── test_quality_tracker.py      # NEW: Contract tests for QualityTracker API
├── integration/
│   └── test_quality_tracking_e2e.py # NEW: End-to-end quality tracking tests
└── unit/
    └── test_quality_tracker.py      # NEW: Unit tests for quality calculations
```

**Structure Decision**: Single project structure (default). This feature extends the existing `llm_orchestrator` and `llm_adapters` modules, following the established pattern of health and cost tracking. Quality metrics will be stored alongside health and cost metrics in the `data/llm_health/` directory, maintaining consistency with the existing file-based persistence approach. The CLI will be extended to expose quality metrics via the existing admin command structure.

## Complexity Tracking

> **Fill ONLY if Constitution Check has violations that must be justified**

No constitution violations identified. The solution follows existing patterns (file-based JSON persistence, pydantic models, atomic writes) without introducing new abstractions or complexity.
