# Implementation Plan: Multi-LLM Provider Support

**Branch**: `011-multi-llm` | **Date**: 2025-11-08 | **Spec**: [spec.md](spec.md)
**Input**: Feature specification from `/specs/011-multi-llm/spec.md`

**Note**: This template is filled in by the `/speckit.plan` command. See `.specify/templates/commands/plan.md` for the execution workflow.

## Summary

Implement a multi-LLM provider architecture that enables the system to use Gemini, Claude (Anthropic), and OpenAI for entity extraction with three orchestration strategies: failover (automatic switching on error), consensus (query multiple providers and merge results), and best-match (select highest confidence result). The system will track provider health, implement circuit breaking for failed providers, monitor costs per provider, and provide admin visibility through CLI commands.

## Technical Context

**Language/Version**: Python 3.12 (established in project)
**Primary Dependencies**: anthropic (Claude SDK), openai (OpenAI SDK)
**Storage**: File-based JSON for provider health metrics and cost tracking
**Testing**: pytest (contract tests for each provider, integration tests for orchestrator strategies)
**Target Platform**: Python CLI application (existing CollabIQ system)
**Project Type**: single
**Performance Goals**: Provider failover completes in <2 seconds, consensus mode queries execute in parallel
**Constraints**: Must maintain backward compatibility with existing LLMProvider interface, provider health metrics persist across application restarts
**Scale/Scope**: Support 3 LLM providers initially (Gemini, Claude, OpenAI), extensible for additional providers

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

### I. Specification-First Development ✅

- [x] Feature specification exists at `specs/011-multi-llm/spec.md`
- [x] Specification includes user scenarios with acceptance criteria (4 user stories)
- [x] Specification includes functional requirements (32 FRs in FR-XXX format)
- [x] Specification includes success criteria (8 SCs in SC-XXX format)
- [x] All requirements are technology-agnostic at specification stage

**Status**: PASS

### II. Incremental Delivery via Independent User Stories ✅

- [x] User stories are prioritized (P1, P2, P3)
- [x] Each user story has independent test criteria
- [x] User Story 1 (P1 - Automatic Failover) constitutes viable MVP
- [x] User stories can be implemented in priority order
- [x] Each story completion results in deployable increment

**User Story Breakdown**:
1. **P1 - Automatic Failover**: System continues processing when primary provider fails (<2s failover)
2. **P2 - Multi-Provider Consensus**: Improves accuracy by 10% through querying multiple providers
3. **P3 - Best-Match Selection**: Optimizes quality by selecting highest-confidence results
4. **P3 - Provider Health Monitoring**: Admin visibility through `collabiq status --detailed`

**Status**: PASS - P1 alone delivers business continuity value

### III. Test-Driven Development (TDD) ✅

Tests are explicitly required in the feature specification.

**Required Tests** (from spec):
- Contract tests for each provider (Gemini, Claude, OpenAI)
- Integration tests for orchestrator strategies (failover, consensus, best-match)
- Failure scenario tests (provider down, timeout, partial failures)

**TDD Discipline**:
- [x] Tests will be written before implementation code
- [x] Tests will initially fail (red phase)
- [x] Implementation proceeds only after failing tests exist
- [x] Refactoring occurs only after tests pass

**Status**: PASS - Tests mandatory, TDD enforced

### IV. Design Artifact Completeness ✅

**Required Artifacts** (generated by this planning phase):
- [x] `plan.md` - This file (completed)
- [x] `research.md` - Phase 0 output (completed)
- [x] `data-model.md` - Phase 1 output (completed)
- [x] `quickstart.md` - Phase 1 output (completed)
- [x] `contracts/` - Phase 1 output (completed)

**Status**: PASS - All artifacts completed

### V. Simplicity & Justification ✅

**Complexity Analysis**:

This feature adds orchestration layer on top of existing `LLMProvider` interface. Key complexity points:

1. **Multiple Provider SDKs**: Adding Claude and OpenAI SDKs
   - **Justification**: Required to meet FR-002 (support 3 providers)
   - **Simpler alternative rejected**: Using only Gemini would not provide failover capability (FR-006)

2. **Orchestrator Pattern**: New layer coordinating multiple providers
   - **Justification**: Required to implement 3 strategies (FR-006, FR-007, FR-008)
   - **Simpler alternative rejected**: Hardcoded if/else logic would not support configurable strategies (FR-009)

3. **Provider Health Tracking**: Separate health metrics persistence
   - **Justification**: Required for circuit breaking (FR-014, FR-015) and admin visibility (FR-028-032)
   - **Simpler alternative rejected**: In-memory only would lose health state on restart

4. **Cost Tracking**: Per-provider token and cost monitoring
   - **Justification**: Explicit requirement (FR-024-027, SC-005)
   - **Simpler alternative rejected**: No tracking would prevent cost visibility

**Status**: PASS - All complexity justified by explicit requirements

## Project Structure

### Documentation (this feature)

```text
specs/011-multi-llm/
├── spec.md              # Feature specification (DONE)
├── plan.md              # This file (/speckit.plan command output - DONE)
├── research.md          # Phase 0 output (/speckit.plan command - DONE)
├── data-model.md        # Phase 1 output (/speckit.plan command - DONE)
├── quickstart.md        # Phase 1 output (/speckit.plan command - DONE)
├── contracts/           # Phase 1 output (/speckit.plan command - DONE)
│   ├── llm-provider-interface.md
│   ├── orchestrator-interface.md
│   └── health-tracker-interface.md
├── checklists/          # Quality validation
│   └── requirements.md  # (DONE)
└── tasks.md             # Phase 2 output (/speckit.tasks command - NOT YET)
```

### Source Code (repository root)

```text
src/
├── llm_provider/                    # Existing LLM abstraction
│   ├── base.py                      # Existing LLMProvider interface
│   ├── types.py                     # Existing ExtractedEntities, ConfidenceScores
│   ├── exceptions.py                # Existing LLM exceptions
│   └── date_utils.py                # Existing date parsing
├── llm_adapters/                    # LLM provider implementations
│   ├── gemini_adapter.py            # Existing Gemini implementation
│   ├── claude_adapter.py            # NEW: Claude/Anthropic implementation
│   ├── openai_adapter.py            # NEW: OpenAI implementation
│   └── health_tracker.py            # NEW: Provider health monitoring
├── llm_orchestrator/                # NEW: Multi-provider orchestration
│   ├── __init__.py
│   ├── orchestrator.py              # Main orchestrator coordinating providers
│   ├── strategies/
│   │   ├── __init__.py
│   │   ├── base.py                  # Strategy interface
│   │   ├── failover.py              # Failover strategy (P1)
│   │   ├── consensus.py             # Consensus strategy (P2)
│   │   └── best_match.py            # Best-match strategy (P3)
│   ├── cost_tracker.py              # Cost tracking per provider
│   └── config.py                    # Orchestrator configuration
├── cli/                             # Existing CLI structure
│   └── llm_commands.py              # NEW: LLM provider CLI commands
└── config/
    └── llm_providers.py             # NEW: Provider configuration

tests/
├── contract/
│   ├── test_llm_provider_interface.py    # Existing contract tests
│   ├── test_claude_adapter_contract.py   # NEW: Claude contract tests
│   ├── test_openai_adapter_contract.py   # NEW: OpenAI contract tests
│   └── test_orchestrator_contract.py     # NEW: Orchestrator contract tests
├── integration/
│   ├── test_gemini_adapter.py            # Existing Gemini integration
│   ├── test_failover_strategy.py         # NEW: Failover integration tests
│   ├── test_consensus_strategy.py        # NEW: Consensus integration tests
│   ├── test_best_match_strategy.py       # NEW: Best-match integration tests
│   └── test_provider_health_tracking.py  # NEW: Health tracking tests
└── unit/
    ├── test_cost_tracker.py              # NEW: Cost tracking unit tests
    └── test_health_tracker.py            # NEW: Health tracker unit tests

data/
└── llm_health/                           # NEW: Provider health persistence
    ├── health_metrics.json               # Current health status per provider
    ├── cost_metrics.json                 # Cost tracking data
    ├── provider_config.json              # Provider configuration
    └── orchestration_config.json         # Orchestration strategy config
```

**Structure Decision**: Single project structure (Option 1) is appropriate. This feature extends the existing CollabIQ codebase by adding new adapters (Claude, OpenAI), a new orchestration layer (`llm_orchestrator/`), and provider health tracking. No web/mobile components required.

## Complexity Tracking

> **Fill ONLY if Constitution Check has violations that must be justified**

No violations - all constitution checks passed. All complexity is justified in Section V above.

---

## Phase 0: Research & Technical Investigation

**Status**: ✅ COMPLETED

All research documented in [research.md](research.md).

### Key Decisions Made

1. **Claude SDK Integration**
   - Package: `anthropic` Python SDK
   - Model: Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
   - Method: Tool calling with forced tool choice for structured JSON
   - Confidence: Prompt-based (Claude doesn't provide native confidence)
   - Cost: $3/M input tokens, $15/M output tokens

2. **OpenAI SDK Integration**
   - Package: `openai` Python SDK
   - Model: GPT-5 Mini (`gpt-5-mini`)
   - Method: Structured Outputs (`response_format` with `json_schema`)
   - Confidence: Prompt-based (like Gemini approach)
   - Cost: $0.25/M input tokens, $2.00/M output tokens

3. **Consensus Algorithm**
   - Strategy: Weighted confidence voting with Dawid-Skene-inspired aggregation
   - Fuzzy matching: Jaro-Winkler distance (threshold 0.85-0.90)
   - Conflict resolution: Weighted voting by `weight × confidence`
   - Tie-breaking: Majority voting → historical performance → abstention

4. **Circuit Breaker Pattern**
   - Pattern: State-based circuit breaker (CLOSED → OPEN → HALF_OPEN)
   - Threshold: 5 consecutive failures (configurable)
   - Timeout: 60 seconds in OPEN state before HALF_OPEN
   - Recovery: 2 successes in HALF_OPEN → CLOSED

5. **Cost Tracking**
   - Method: Per-token pricing with provider-specific rates
   - Storage: JSON file (`data/llm_health/cost_metrics.json`)
   - Calculation: `(input_tokens × input_price) + (output_tokens × output_price)`

6. **Persistence**
   - Format: JSON files in `data/llm_health/` directory
   - Atomic writes: Temp file + rename pattern
   - File locking: Optional for multi-process scenarios

---

## Phase 1: Design Artifacts

**Status**: ✅ COMPLETED

All design artifacts generated and documented.

### Data Model

Documented in [data-model.md](data-model.md).

**Entities defined**:
1. **LLM Provider Configuration** - Provider settings, API keys, pricing
2. **Provider Health Metrics** - Success/failure tracking, circuit breaker state
3. **Extraction Result with Provider Metadata** - Extended extraction results with token usage and cost
4. **Orchestration Strategy Configuration** - Strategy selection and parameters
5. **Cost Metrics Summary** - Cumulative cost tracking per provider

### API Contracts

Documented in [contracts/](contracts/).

**Contracts defined**:
1. **[LLM Provider Interface](contracts/llm-provider-interface.md)**
   - `extract_entities(email_text: str) -> ExtractedEntities`
   - Exception hierarchy (LLMAPIError, LLMRateLimitError, etc.)
   - Contract tests for all providers

2. **[Orchestrator Interface](contracts/orchestrator-interface.md)**
   - `extract_entities(email_text: str, strategy: str | None) -> ExtractedEntities`
   - `get_provider_status() -> dict[str, ProviderStatus]`
   - `set_strategy(strategy_type: str) -> None`
   - `test_provider(provider_name: str) -> bool`
   - Strategy implementations (Failover, Consensus, Best-Match)

3. **[Health Tracker Interface](contracts/health-tracker-interface.md)**
   - `is_healthy(provider_name: str) -> bool`
   - `record_success(provider_name: str, response_time_ms: float) -> None`
   - `record_failure(provider_name: str, error_message: str) -> None`
   - `get_metrics(provider_name: str) -> ProviderHealthMetrics`
   - Circuit breaker state machine

### Usage Guide

Documented in [quickstart.md](quickstart.md).

**Covers**:
- Installation and API key setup
- Basic usage scenarios (failover, consensus, best-match)
- CLI commands (`collabiq llm status`, `test`, `set-strategy`)
- Configuration files and structure
- Monitoring and troubleshooting
- Best practices and cost optimization

---

## Implementation Roadmap

### User Story 1 (P1): Automatic Failover - MVP

**Goal**: System continues processing emails when primary provider fails

**Components to Build**:
1. `ClaudeAdapter` - Implements LLMProvider interface using Claude SDK
2. `OpenAIAdapter` - Implements LLMProvider interface using OpenAI SDK
3. `HealthTracker` - Basic health tracking and circuit breaking
4. `FailoverStrategy` - Sequential provider attempts
5. `LLMOrchestrator` - Orchestrator with failover strategy
6. Contract tests for Claude and OpenAI adapters
7. Integration tests for failover strategy

**Success Criteria**:
- SC-001: System continues processing when primary provider unavailable ✅
- SC-002: Failover completes in <2 seconds ✅
- SC-006: All providers pass contract tests ✅
- SC-007: Unhealthy providers automatically bypassed ✅

**Estimated Effort**: 3-4 days

### User Story 2 (P2): Multi-Provider Consensus

**Goal**: Improve extraction accuracy by querying multiple providers

**Components to Build**:
1. `ConsensusStrategy` - Parallel queries with result merging
2. Fuzzy matching logic (Jaro-Winkler)
3. Weighted voting algorithm
4. Integration tests for consensus strategy

**Success Criteria**:
- SC-003: Consensus mode improves accuracy by 10% ✅

**Estimated Effort**: 2 days

### User Story 3 (P3): Best-Match Selection

**Goal**: Select highest-confidence result across providers

**Components to Build**:
1. `BestMatchStrategy` - Parallel queries with confidence comparison
2. Aggregate confidence calculation
3. Integration tests for best-match strategy

**Success Criteria**:
- SC-008: Best-match selects highest-confidence result ✅

**Estimated Effort**: 1 day

### User Story 4 (P3): Provider Health Monitoring

**Goal**: Admin visibility into provider health and costs

**Components to Build**:
1. `CostTracker` - Token usage and cost calculation
2. CLI commands (`llm status`, `llm test`, `llm set-strategy`)
3. Enhanced health metrics display
4. Cost reporting functionality

**Success Criteria**:
- SC-004: Admin can view provider health via CLI ✅
- SC-005: Cost per email tracked and reported ✅

**Estimated Effort**: 1-2 days

---

## Testing Strategy

### Test-Driven Development Workflow

**Red → Green → Refactor**

1. **RED**: Write failing test first
   ```python
   def test_claude_adapter_extracts_entities():
       adapter = ClaudeAdapter(api_key="test_key")
       result = adapter.extract_entities("test email")
       assert isinstance(result, ExtractedEntities)
   ```

2. **GREEN**: Implement minimal code to pass test
   ```python
   class ClaudeAdapter(LLMProvider):
       def extract_entities(self, email_text: str) -> ExtractedEntities:
           # Minimal implementation
           pass
   ```

3. **REFACTOR**: Clean up implementation
   - Extract methods
   - Remove duplication
   - Improve readability

### Test Coverage Requirements

**Contract Tests** (100% required):
- All providers must pass `test_llm_provider_interface.py`
- Provider-specific contract tests for Claude and OpenAI
- Orchestrator contract tests

**Integration Tests** (user journey coverage):
- Failover strategy with provider failures
- Consensus strategy with conflicting results
- Best-match strategy with varying confidence
- Health tracking with recovery scenarios

**Unit Tests** (logic verification):
- Cost calculation accuracy
- Health metrics updates
- Circuit breaker state transitions
- Fuzzy matching algorithm

---

## Dependencies

### External Dependencies (New)

```toml
# Add to pyproject.toml
dependencies = [
    # ... existing dependencies
    "anthropic>=0.40.0",     # Claude SDK
    "openai>=1.102.0",        # OpenAI SDK
]
```

### Internal Dependencies

- Existing `LLMProvider` interface (no changes)
- Existing `ExtractedEntities` model (no changes)
- Existing error handling infrastructure (`retry_with_backoff`, circuit breakers)
- Existing CLI framework (`typer`, `rich`)

---

## Risk Assessment

### Technical Risks

1. **Provider API Changes**
   - **Risk**: Claude or OpenAI change their API schemas
   - **Mitigation**: Contract tests detect breaking changes early, version pinning

2. **Cost Overruns**
   - **Risk**: Consensus mode queries multiple providers, increasing costs
   - **Mitigation**: Failover as default, cost tracking and alerts, budget thresholds

3. **Performance Degradation**
   - **Risk**: Parallel queries add latency for consensus/best-match
   - **Mitigation**: Timeout configuration, performance benchmarks, P1 focuses on failover

4. **Confidence Score Calibration**
   - **Risk**: Prompt-based confidence scores not well-calibrated across providers
   - **Mitigation**: Research shows reasonable calibration, validation against test set

### Operational Risks

1. **Provider Outages**
   - **Risk**: All providers fail simultaneously
   - **Mitigation**: Circuit breaker prevents cascading failures, health monitoring alerts

2. **API Key Management**
   - **Risk**: API keys leaked or expired
   - **Mitigation**: Environment variable storage, `.env` in `.gitignore`, auth error handling

3. **State Corruption**
   - **Risk**: Health/cost metrics JSON files corrupted
   - **Mitigation**: Atomic writes (temp + rename), validation on load, backup strategies

---

## Next Steps

After this planning phase:

1. **Generate Tasks** (`/speckit.tasks`):
   - Create dependency-ordered `tasks.md`
   - Organize tasks by user story (P1 → P2 → P3)
   - Define acceptance criteria per task

2. **Begin Implementation** (`/speckit.implement`):
   - Start with P1 (Failover strategy)
   - Follow TDD discipline strictly
   - Complete P1 before moving to P2

3. **Continuous Validation**:
   - Run contract tests after each provider implementation
   - Validate health tracking and cost metrics
   - Monitor performance benchmarks

4. **Quality Gates**:
   - All P1 tests pass before P2 starts
   - All contract tests pass before merging
   - Constitution checks verified throughout

---

## References

**Feature Documentation**:
- [spec.md](spec.md) - Feature specification
- [research.md](research.md) - Technical research findings
- [data-model.md](data-model.md) - Entity definitions
- [quickstart.md](quickstart.md) - Usage guide
- [contracts/](contracts/) - API contracts

**External Resources**:
- Anthropic SDK: https://github.com/anthropics/anthropic-sdk-python
- OpenAI SDK: https://github.com/openai/openai-python
- Dawid-Skene Model: Research paper on consensus voting
- Jaro-Winkler Similarity: String matching algorithm
